---
id: lexer
title: Lexer
---

## Token

è¯æ³•åˆ†æå™¨ï¼Œä¹Ÿç§°ä¸ºåˆ†è¯å™¨ (tokenizer) æˆ–æ‰«æå™¨ (scanner)ï¼Œè´Ÿè´£å°†æºä»£ç æ–‡æœ¬è½¬æ¢ä¸ºè¯å…ƒï¼ˆtokensï¼‰ã€‚
è¿™äº› token ç¨åå°†è¢«è§£æå™¨æ¶ˆè€—ï¼Œå› æ­¤æˆ‘ä»¬ä¸å¿…æ‹…å¿ƒåŸå§‹æ–‡æœ¬ä¸­çš„ç©ºæ ¼å’Œæ³¨é‡Šã€‚

è®©æˆ‘ä»¬ä»ç®€å•çš„å¼€å§‹ï¼šå°†å•ä¸ª `+` æ–‡æœ¬è½¬æ¢ä¸ºä¸€ä¸ª tokenã€‚

```rust
#[derive(Debug, Clone, Copy, PartialEq)]
pub struct Token {
    /// æ ‡è®°ç±»å‹
    pub kind: Kind,

    /// æºæ–‡æœ¬ä¸­çš„èµ·å§‹åç§»é‡
    pub start: usize,

    /// æºæ–‡æœ¬ä¸­çš„ç»“æŸåç§»é‡
    pub end: usize,
}

#[derive(Debug, Clone, Copy, PartialEq)]
pub enum Kind {
    Eof, // æ–‡ä»¶ç»“æŸ
    Plus,
}
```

å•ä¸ª `+` ä¼šè¾“å‡ºï¼š

```
[
    Token { kind: Kind::Plus, start: 0, end: 1 },
    Token { kind: Kind::Eof,  start: 1, end: 1 }
]
```

ä¸ºäº†éå†å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬å¯ä»¥å¦‚åŒå†™ C ä»£ç é‚£æ ·ç»´æŠ¤ä¸€ä¸ªç´¢å¼•ï¼›
åˆæˆ–è€…æˆ‘ä»¬å¯ä»¥æŸ¥çœ‹ [å­—ç¬¦ä¸²æ–‡æ¡£](https://doc.rust-lang.org/std/primitive.str.html#)
å¹¶æ‰¾åˆ°ä¸€ä¸ª [`Chars`](https://doc.rust-lang.org/std/str/struct.Chars.html) è¿­ä»£å™¨æ¥ä½¿ç”¨ã€‚

:::info
`Chars` è¿­ä»£å™¨æŠ½è±¡æ‰äº†ç´¢å¼•çš„ç»´æŠ¤å’Œè¾¹ç•Œæ£€æŸ¥ç­‰ç»†èŠ‚ï¼Œè®©æˆ‘ä»¬å†™ä»£ç çš„æ—¶å€™å……æ»¡å®‰å…¨æ„Ÿã€‚

å½“æˆ‘ä»¬è°ƒç”¨ `chars.next()` æ—¶ï¼Œå®ƒä¼šè¿”å› `Option<char>`ã€‚
ä½†è¯·æ³¨æ„ï¼Œ`char` ä¸æ˜¯ 0-255 çš„ ASCII å€¼ï¼Œå®ƒæ˜¯ä¸€ä¸ªèŒƒå›´åœ¨ 0 åˆ° 0x10FFFF ä¹‹é—´çš„ utf8 Unicode ç ç‚¹å€¼ã€‚
:::

è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªåˆæ­¥çš„è¯æ³•åˆ†æå™¨æŠ½è±¡

```rust
use std::str::Chars;

struct Lexer<'a> {
    /// æºæ–‡æœ¬
    source: &'a str,

    /// å‰©ä½™çš„å­—ç¬¦
    chars: Chars<'a>
}

impl<'a> Lexer<'a> {
    pub fn new(source: &'a str) -> Self {
        Self {
            source,
            chars: source.chars()
        }
    }
}
```

:::info
è¿™é‡Œçš„ç”Ÿå‘½å‘¨æœŸ `'a` è¡¨ç¤ºè¿­ä»£å™¨å¼•ç”¨äº†æŸä¸ªåœ°æ–¹ã€‚åœ¨è¿™é‡Œï¼Œå®ƒå¼•ç”¨äº†ä¸€ä¸ª `&'a str`ã€‚
:::

è¦å°†æºæ–‡æœ¬è½¬æ¢ä¸º token ï¼Œåªéœ€ä¸æ–­è°ƒç”¨ `chars.next()` å¹¶å¯¹è¿”å›çš„ `char`è¿›è¡Œæ¨¡å¼åŒ¹é…ã€‚
æœ€åä¸€ä¸ª token å°†å§‹ç»ˆæ˜¯ `Kind::Eof`ã€‚

ç½‘ç»œè¿æ¥è¶…æ—¶ï¼Œæ­£åœ¨è‡ªåŠ¨é‡è¯•~

```rust
impl<'a> Lexer<'a> {
    fn read_next_kind(&mut self) -> Kind {
        while let Some(c) = self.chars.next() {
            match c {
              '+' => return Kind::Plus,
              _ => {}
            }
        }
        Kind::Eof
    }

    fn read_next_token(&mut self) -> Token {
        let start = self.offset();
        let kind = self.read_next_kind();
        let end = self.offset();
        Token { kind, start, end }
    }

    /// è·å–ä»æºæ–‡æœ¬ä¸­çš„åç§»é•¿åº¦ï¼Œä»¥ UTF-8 å­—èŠ‚è¡¨ç¤º
    fn offset(&self) -> usize {
        self.source.len() - self.chars.as_str().len()
    }
}
```

åœ¨ `fn offset` ä¸­ï¼Œ`.len()` å’Œ `.as_str().len()` æ–¹æ³•çœ‹èµ·æ¥åƒæ˜¯ O(n) çš„ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬è¿›ä¸€æ­¥çœ‹çœ‹æ˜¯å¦å¦‚æ­¤ã€‚

[`.as_str()`](https://doc.rust-lang.org/src/core/str/iter.rs.html#112) è¿”å›ä¸€ä¸ªæŒ‡å‘å­—ç¬¦ä¸²åˆ‡ç‰‡çš„æŒ‡é’ˆ

```rust reference
https://github.com/rust-lang/rust/blob/b998821e4c51c44a9ebee395c91323c374236bbb/library/core/src/str/iter.rs#L112-L115
```

ä¸€ä¸ªåˆ‡ç‰‡ ([slice](https://doc.rust-lang.org/std/slice/index.html))æ˜¯ä½œä¸ºæŒ‡é’ˆå’Œé•¿åº¦è¡¨ç¤ºçš„å†…å­˜å—çš„è§†å›¾ã€‚
`.len()` æ–¹æ³•è¿”å›åˆ‡ç‰‡å†…éƒ¨å­˜å‚¨çš„å…ƒæ•°æ®

```rust reference
https://github.com/rust-lang/rust/blob/b998821e4c51c44a9ebee395c91323c374236bbb/library/core/src/str/mod.rs#L157-L159
```

```rust reference
https://github.com/rust-lang/rust/blob/b998821e4c51c44a9ebee395c91323c374236bbb/library/core/src/str/mod.rs#L323-L325
```

```rust reference
https://github.com/rust-lang/rust/blob/b998821e4c51c44a9ebee395c91323c374236bbb/library/core/src/slice/mod.rs#L129-L138
```

ä¸Šé¢æåˆ°çš„è¿™äº›æ–¹æ³•åœ¨ç¼–è¯‘ä¹‹åéƒ½ä¼šæˆä¸ºå•æ¬¡æ•°æ®è®¿é—®ï¼Œå› æ­¤ `.as_str().len()` å®é™…ä¸Šæ˜¯ O(1)çš„ã€‚

## Peek

To tokenize multi-character operators such as `++` or `+=`, a helper function `peek` is required:

```rust
fn peek(&self) -> Option<char> {
    self.chars.clone().next()
}
```

We don't want to advance the original `chars` iterator so we clone the iterator and advance the index.

:::info
The `clone` is cheap if we dig into the [source code](https://doc.rust-lang.org/src/core/slice/iter.rs.html#148-152),
it just copies the tracking and boundary index.

```rust reference
https://github.com/rust-lang/rust/blob/b998821e4c51c44a9ebee395c91323c374236bbb/library/core/src/slice/iter.rs#L148-L152
```

:::

The difference between `peek` and `chars.next()` is the former will always return the **same** next `char`,
while the later will move forward and return a different `char`.

To demonstrate, consider the string `abc`:

- repeated `peek()` call returns `Some(a)`, `Some(a)`, `Some(a)`, ...
- repeated `chars.next()` call returns `Some('a')`, `Some('b')`, `Some('c')`, `None`.

Equipped with `peek`, tokenizing `++` and `+=` are just nested if statements.

Here is a real-world implementation from [jsparagus](https://github.com/mozilla-spidermonkey/jsparagus):

```rust reference
https://github.com/mozilla-spidermonkey/jsparagus/blob/master/crates/parser/src/lexer.rs#L1769-L1791
```

The above logic applies to all operators, so let us expand our knowledge on lexing JavaScript.

## JavaScript

A lexer written in Rust is rather boring, it feels like writing C code
where we write long chained if statements and check for each `char` and then return the respective token.

The real fun begins when we start lexing for JavaScript.

Let's open up the [ECMAScript Language Specification](https://tc39.es/ecma262/) and re-learn JavaScript.

:::caution
I still remember the first time I opened up the specification and went into a little corner
and cried in agony because it feels like reading foreign text with jargons everywhere.
So head over to my [guide on reading the specification](/blog/ecma-spec) if things don't make sense.
:::

### Comments

Comments have no semantic meaning, they can be skipped if we are writing a runtime,
but they need to be taken into consideration if we are writing a linter or a bundler.

### Identifiers and Unicode

We mostly code in ASCII,
but [Chapter 11 ECMAScript Language: Source Text](https://tc39.es/ecma262/#sec-ecmascript-language-source-code)
states the source text should be in Unicode.
And [Chapter 12.6 Names and Keywords](https://tc39.es/ecma262/#sec-names-and-keywords)
states the identifiers are interpreted according to the Default Identifier Syntax given in Unicode Standard Annex #31.
In detail:

```markup
IdentifierStartChar ::
    UnicodeIDStart

IdentifierPartChar ::
    UnicodeIDContinue

UnicodeIDStart ::
    any Unicode code point with the Unicode property â€œID_Startâ€

UnicodeIDContinue ::
    any Unicode code point with the Unicode property â€œID_Continueâ€
```

This means that we can write `var à² _à² ` but not `var ğŸ¦€`,
`à² ` has the Unicode property "ID_Start" while `ğŸ¦€` does not.

:::info

I published the [unicode-id-start](https://crates.io/crates/unicode-id-start) crate for this exact purpose.
`unicode_id_start::is_id_start(char)` and `unicode_id_start::is_id_continue(char)` can be called to check Unicode.

:::

### Keywords

All the [keywords](https://tc39.es/ecma262/#sec-keywords-and-reserved-words) such as `if`, `while` and `for`
need to be tokenized and interpreted as a whole.
They need to be added to the token kind enum so we don't have to make string comparisons in the parser.

```rust
pub enum Kind {
    Identifier,
    If,
    While,
    For
}
```

:::caution
`undefined` is not a keyword, it is unnecessary to add it here.
:::

Tokenizing keywords will just be matching the identifier from above.

```rust
fn match_keyword(&self, ident: &str) -> Kind {
    // all keywords are 1 <= length <= 10
    if ident.len() == 1 || ident.len() > 10 {
        return Kind::Identifier;
    }
    match ident {
        "if" => Kind::If,
        "while" => Kind::While,
        "for" => Kind::For,
        _ => Kind::Identifier
    }
}
```

### Token Value

We often need to compare identifiers, numbers and strings in later stages of the compiler phases,
for example testing against identifiers inside a linter,

These values are currently in plain source text,
let's convert them to Rust types so they are easier to work with.

```rust
pub enum Kind {
    Eof, // end of file
    Plus,
    // highlight-start
    Identifier,
    Number,
    String,
    // highlight-end
}

#[derive(Debug, Clone, Copy, PartialEq)]
pub struct Token {
    /// Token Type
    pub kind: Kind,

    /// Start offset in source
    pub start: usize,

    /// End offset in source
    pub end: usize,

    // highlight-next-line
    pub value: TokenValue,
}

#[derive(Debug, Clone, PartialEq)]
pub enum TokenValue {
    None,
    Number(f64),
    String(String),
}
```

When an identifier `foo` or string `"bar"` is tokenized , we get

```markup
Token { kind: Kind::Identifier, start: 0, end: 2, value: TokenValue::String("foo") }
Token { kind: Kind::String, start: 0, end: 4, value: TokenValue::String("bar") }
```

To convert them to Rust strings, call `let s = self.source[token.start..token.end].to_string()`
and save it with `token.value = TokenValue::String(s)`.

When we tokenized a number `1.23`, we get a token with `Token { start: 0, end: 3 }`.
To convert it to Rust `f64`, we can use the string [`parse`](https://doc.rust-lang.org/std/primitive.str.html#method.parse)
method by calling `self.source[token.start..token.end].parse::<f64>()`, and then save the value into `token.value`.
For binary, octal and integers, an example of their parsing techniques can be found in [jsparagus](https://github.com/mozilla-spidermonkey/jsparagus/blob/master/crates/parser/src/numeric_value.rs).

## Rust Optimizations

### Smaller Tokens

It is tempting to put the token values inside the `Kind` enum and aim for simpler and safer code:

```rust
pub enum Kind {
    Number(f64),
    String(String),
}
```

But it is known that the byte size of a Rust enum is the union of all its variants.
This enum packs a lot of bytes compared to the original enum, which has only 1 byte.
There will be heavy usages of this `Kind` enum in the parser,
dealing with a 1 byte enum will obviously be faster than a multi-byte enum.

### String Interning

It is not performant to use `String` in compilers, mainly due to:

- `String` is a heap allocated object
- String comparison is an O(n) operation

[String Interning](https://en.wikipedia.org/wiki/String_interning) solves these problems by
storing only one copy of each distinct string value with a unique identifier in a cache.
There will only be one heap allocation per distinct identifier or string, and string comparisons become O(1).

There are lots of string interning libraries on [crates.io](https://crates.io/search?q=string%20interning)
with different pros and cons.

A sufficient starting point is to use [`string-cache`](https://crates.io/crates/string_cache),
it has an `Atom` type and a compile time `atom!("string")` interface.

With `string-cache`, `TokenValue` becomes

```rust
#[derive(Debug, Clone, PartialEq)]
pub enum TokenValue {
    None,
    Number(f64),
    // highlight-next-line
    String(Atom),
}
```

and string comparison becomes `matches!(value, TokenValue::String(atom!("string")))`.
